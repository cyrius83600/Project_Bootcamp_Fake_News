{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def DetectWords(df_func):\n",
    "    df_func['tokenize'] = df_func['text'].apply(Tokenize)\n",
    "    df_func['tokenized_text'] = df_func['tokenize'].apply(lambda tokens:' '.join(tokens).lower())\n",
    "    df_cleaned_func = df_func.loc[:,['tokenized_text','label']]\n",
    "    X_func = df_cleaned_func['tokenized_text']\n",
    "    vectorizer_func = TfidfVectorizer()\n",
    "    X_cleaned_func = vectorizer_func.fit_transform(X_func)\n",
    "    for feature in vectorizer_func.get_feature_names_out():\n",
    "        print(vectorizer_func.get_feature_names_out())\n",
    "    return X_cleaned_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_kaggle = pd.read_csv('Fake_Kaggle/train.csv')\n",
    "df_fake_kaggle = df_fake_kaggle[(df_fake_kaggle['class'] == 'Fake') | (df_fake_kaggle['class'] == 'Real')]\n",
    "df_fake_kaggle['class'] = df_fake_kaggle['class'].apply(lambda x : 0 if x == 'Real' else 1)\n",
    "df_val_fake_kaggle = pd.DataFrame(pd.read_csv('Fake_Kaggle/test.csv').text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFileTest(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df.loc[:,['text','label']]\n",
    "    df.dropna(inplace=True)\n",
    "    df['label'] = df['label'].apply(lambda x : 0 if x == 1 else 1)\n",
    "    return df\n",
    "\n",
    "df_train_fake_news_kaggle = GetFileTest('Fake_News_Kaggle/train.csv')\n",
    "df_val_fake_news_kaggle = pd.read_csv('Fake_News_Kaggle/test.csv').loc[:,['text']]\n",
    "df_val_fake_news_kaggle.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36078/4075386623.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping['label'] = df_liar_mapping.IsFakeNews.apply(lambda x: 0 if x=='false' else 1)\n",
      "/tmp/ipykernel_36078/4075386623.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping.drop('IsFakeNews',axis=1,inplace=True)\n",
      "/tmp/ipykernel_36078/4075386623.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping['label'] = df_liar_mapping.IsFakeNews.apply(lambda x: 0 if x=='false' else 1)\n",
      "/tmp/ipykernel_36078/4075386623.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping.drop('IsFakeNews',axis=1,inplace=True)\n",
      "/tmp/ipykernel_36078/4075386623.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping['label'] = df_liar_mapping.IsFakeNews.apply(lambda x: 0 if x=='false' else 1)\n",
      "/tmp/ipykernel_36078/4075386623.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_liar_mapping.drop('IsFakeNews',axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def GetMappingLiarDataset(files_name):\n",
    "    dfs = []\n",
    "    for file_name in files_name:\n",
    "        df_liar = pd.read_csv(file_name,delimiter='\\t')\n",
    "        df_liar_mapping = df_liar.iloc[:,[1,2]]\n",
    "        df_liar_mapping.columns=['IsFakeNews','text']\n",
    "        df_liar_mapping['label'] = df_liar_mapping.IsFakeNews.apply(lambda x: 0 if x=='false' else 1)\n",
    "        df_liar_mapping.drop('IsFakeNews',axis=1,inplace=True)\n",
    "        dfs.append(df_liar_mapping)\n",
    "    df_test = pd.concat([dfs[1],dfs[2]],axis=0)\n",
    "    return (dfs[0],df_test)\n",
    "df_train_liar_dataset, df_test_liar_datasset = GetMappingLiarDataset(['Liar_Dataset/train.tsv','Liar_Dataset/test.tsv','Liar_Dataset/valid.tsv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_misinfo = pd.read_csv('Misinfo/DataSet_Misinfo_FAKE.csv')\n",
    "df_fake_misinfo['label'] = df_fake_misinfo['text'].apply(lambda x:1)\n",
    "df_fake_misinfo = df_fake_misinfo.loc[:,['text','label']]\n",
    "df_true_misinfo = pd.read_csv('Misinfo/DataSet_Misinfo_TRUE.csv')\n",
    "df_true_misinfo['label'] = df_true_misinfo['text'].apply(lambda x:0)\n",
    "df_true_misinfo = df_true_misinfo.loc[:,['text','label']]\n",
    "df_true_misinfo.dropna(inplace=True)\n",
    "df_misinfo = pd.concat([df_fake_misinfo,df_true_misinfo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('Fake_Real_News_Kaggle/Fake.csv')\n",
    "df_fake = pd.DataFrame(df_fake.text,columns=['text'])\n",
    "df_fake['label'] = df_fake.text.apply(lambda x : 1)\n",
    "df_real = pd.read_csv('Fake_Real_News_Kaggle/True.csv')\n",
    "df_real = pd.DataFrame(df_real.text)\n",
    "df_real['label'] = df_real.text.apply(lambda x : 0)\n",
    "df_real_news_kaggle = pd.concat([df_fake,df_real],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_welfake = pd.read_csv('Wel_Fake_Kaggle/WELFake_Dataset.csv')\n",
    "df_welfake = df_welfake.loc[:,['text','label']]\n",
    "df_welfake.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_input = pd.read_csv('Input_Fake_News/train.csv',delimiter=';')\n",
    "df_train_input = df_train_input.loc[:,['text','label']]\n",
    "df_test_input = pd.read_csv('Input_Fake_News/test.csv',delimiter=';')\n",
    "df_test_input = df_test_input.loc[:,['text','label']]\n",
    "df_evaluation_input = pd.read_csv('Input_Fake_News/evaluation.csv',delimiter=';')\n",
    "df_evaluation_input = df_evaluation_input.loc[:,['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = pd.concat([df_train_fake_news_kaggle, df_train_liar_dataset, df_test_liar_datasset, df_misinfo, df_real_news_kaggle, df_welfake, df_train_input, df_test_input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = df_final_train[df_final_train['label'] == 1]\n",
    "df_true = df_final_train[df_final_train['label'] == 0]\n",
    "np.random.seed(42)\n",
    "indexes_fake = np.random.randint(0,df_fake.shape[0],12500)\n",
    "indexes_true = np.random.randint(0,df_true.shape[0],10000)\n",
    "df_fake = df_fake.iloc[indexes_fake]\n",
    "df_true = df_true.iloc[indexes_true]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_fake,df_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseLine = 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "english_words = set(words.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNonAlpha(text):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    test = False\n",
    "    for char in text:\n",
    "        if char != ' ' and char not in alphabet:\n",
    "            test = True\n",
    "    if test == True:\n",
    "        return ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output\n",
    "\n",
    "def Tokenize(text):\n",
    "    text = text.lower()\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_liste = [token for token in word_tokenize(text) if token.isalpha() and token not in stop_words]\n",
    "    test = False\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for mot in token_liste:\n",
    "        for char in mot:\n",
    "            if char not in alphabet:\n",
    "                test = True\n",
    "    if test == False:\n",
    "        final_liste = [token for token in token_liste]\n",
    "        tags = nltk.pos_tag(final_liste)\n",
    "        wordnet_input = get_wordnet_pos(tags)\n",
    "        lem_tokens = [lemmatizer.lemmatize(t,tag) for t,tag in wordnet_input]\n",
    "        return lem_tokens\n",
    "    return [' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cyrius8360/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize'] = df['text'].apply(Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['tokenize'].apply(lambda tokens:' '.join(tokens).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tokenized_text'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66453</th>\n",
       "      <td>mint press news tue oct utc inmate make sandwi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13013</th>\n",
       "      <td>kabul reuters islamic state claim responsibili...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30750</th>\n",
       "      <td>donald trump describe many sound like comic bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62491</th>\n",
       "      <td>rmuse sit oct pm take conspiracy theory obstru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43627</th>\n",
       "      <td>trey gowdy chairman benghazi select committee ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33297</th>\n",
       "      <td>amsterdam reuters least two people kill severa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34518</th>\n",
       "      <td>asuncion reuters paraguayan senator mario abdo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36870</th>\n",
       "      <td>reuters canadian prime minister justin trudeau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>since obamacare become law new job job</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>comment crosstalk bullhorns electioneer leave ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21704 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tokenized_text  label\n",
       "66453  mint press news tue oct utc inmate make sandwi...      1\n",
       "13013  kabul reuters islamic state claim responsibili...      1\n",
       "30750  donald trump describe many sound like comic bo...      1\n",
       "62491  rmuse sit oct pm take conspiracy theory obstru...      1\n",
       "43627  trey gowdy chairman benghazi select committee ...      1\n",
       "...                                                  ...    ...\n",
       "33297  amsterdam reuters least two people kill severa...      0\n",
       "34518  asuncion reuters paraguayan senator mario abdo...      0\n",
       "36870  reuters canadian prime minister justin trudeau...      0\n",
       "3598              since obamacare become law new job job      0\n",
       "7350   comment crosstalk bullhorns electioneer leave ...      0\n",
       "\n",
       "[21704 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df.loc[:,['tokenized_text','label']]\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned['tokenized_text']\n",
    "y = df_cleaned['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaahhh' ... 'zytsov' 'zyuganov' 'zzzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_cleaned = vectorizer.fit_transform(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.742455655378945, 0.7814698983580923)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train, y_train)\n",
    "y_pred = lreg.predict(X_test)\n",
    "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga', 'newton-cg', 'sag'],\n",
    "    'max_iter': [100000]\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7373333333333333, 0.7711076684740512)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svc = SVC()\n",
    "# svc.fit(X_train, y_train)\n",
    "# y_pred = svc.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5842222222222222, 0.7181804488627807)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(X_train, y_train)\n",
    "# y_pred_knn = knn.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred_knn), f1_score(y_test, y_pred_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
